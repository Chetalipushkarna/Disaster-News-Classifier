{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKvcypr_smvb",
        "outputId": "554f1122-3020-430a-de0a-2ac65c1378a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9672131147540983\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.79      0.85       400\n",
            "           1       0.91      0.96      0.93       798\n",
            "           2       0.99      1.00      1.00      2401\n",
            "\n",
            "    accuracy                           0.97      3599\n",
            "   macro avg       0.94      0.92      0.93      3599\n",
            "weighted avg       0.97      0.97      0.97      3599\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load your CSV files\n",
        "file1 = \"/content/td1.csv\"\n",
        "file2 = \"/content/td2.csv\"\n",
        "file3 = \"/content/td3.csv\"\n",
        "\n",
        "# Read the CSV files into dataframes\n",
        "df1 = pd.read_csv(file1)\n",
        "df2 = pd.read_csv(file2)\n",
        "df3 = pd.read_csv(file3)\n",
        "\n",
        "# Concatenate the dataframes into one\n",
        "data = pd.concat([df1, df2, df3], ignore_index=True)\n",
        "\n",
        "# Preprocessing: text cleaning, tokenization, and label encoding\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):  # Check for missing values (NaN)\n",
        "        return \"\"  # Return an empty string for missing values\n",
        "    # Remove non-alphanumeric characters and convert to lowercase\n",
        "    text = \" \".join(word.lower() for word in str(text).split() if word.isalpha())\n",
        "    # Tokenization\n",
        "    tokens = text.split()\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "data['content'] = data['content'].apply(preprocess_text)\n",
        "\n",
        "# Encode the 'label' column (e.g., 'yes' and 'no' to 1 and 0)\n",
        "label_encoder = LabelEncoder()\n",
        "data['label'] = label_encoder.fit_transform(data['label'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X = data['content']\n",
        "y = data['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a text classification pipeline (you can customize this)\n",
        "text_clf = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('select_best', SelectKBest(chi2, k=2000)),\n",
        "    ('clf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "text_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = text_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# If needed, you can perform hyperparameter tuning using GridSearchCV\n",
        "param_grid = {\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "    'clf__n_estimators': [100, 200, 300],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(text_clf, param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh4c5WwglVMq",
        "outputId": "f141b6f5-3755-441d-97d2-e7fe9281e403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkKw6QUbb-RE",
        "outputId": "3488e547-8cef-4ccb-b27c-5f84fb8c70ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas scikit-learn nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JofL0XWHcG5o",
        "outputId": "a6edc518-bf6a-40b6-fa80-53a3885746f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the Disaster News Classification Chatbot!\n",
            "Enter a news statement (or 'exit' to quit): there was Tsunami of fans outside the cricket stadium\n",
            "Chatbot Response: no\n",
            "Enter a news statement (or 'exit' to quit): Odisha was hit by a Tsunami due to rise in water levels\n",
            "Chatbot Response: yes\n",
            "Enter a news statement (or 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "## USING RandomForest\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "import joblib  # For model saving\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "data_files = [\"/content/td1.csv\", \"/content/td2.csv\", \"/content/td3.csv\"]\n",
        "dataframes = []\n",
        "\n",
        "for file in data_files:\n",
        "    df = pd.read_csv(file)\n",
        "    dataframes.append(df)\n",
        "\n",
        "data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Preprocessing: text cleaning, tokenization, and label encoding\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = \" \".join(word.lower() for word in str(text).split() if word.isalpha())\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "data['content'] = data['content'].apply(preprocess_text)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "data['label'] = label_encoder.fit_transform(data['label'])\n",
        "\n",
        "# Train a text classification model\n",
        "X = data['content']\n",
        "y = data['label']\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "select_best = SelectKBest(chi2, k=2000)\n",
        "X_new = select_best.fit_transform(X_tfidf, y)\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_new, y)\n",
        "\n",
        "# Save the trained model to a file\n",
        "joblib.dump(model, \"disaster_classification_model.pkl\")\n",
        "\n",
        "# Define a function to classify news\n",
        "def classify_news(news_text):\n",
        "    news_text = preprocess_text(news_text)\n",
        "    news_tfidf = tfidf_vectorizer.transform([news_text])\n",
        "    news_selected = select_best.transform(news_tfidf)\n",
        "    prediction = model.predict(news_selected)\n",
        "    return label_encoder.inverse_transform(prediction)[0]\n",
        "\n",
        "# Console-based chatbot\n",
        "print(\"Welcome to the Disaster News Classification Chatbot!\")\n",
        "while True:\n",
        "    user_input = input(\"Enter a news statement (or 'exit' to quit): \")\n",
        "    user_input=user_input.lower()\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    classification = classify_news(user_input)\n",
        "    print(f\"Chatbot Response: {classification}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}